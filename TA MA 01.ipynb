{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics - Mantatory Assignment 01\n",
    "## Student: Alin Cristian Preda\n",
    "### Programme: Data Science\n",
    "#### Copenhagen, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from collections import Counter, defaultdict\n",
    "counts=Counter(brown.words()) # number of appeareance of each word in the corpus \n",
    "total_count=(len(brown.words())) # number of all words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in counts:\n",
    "    counts[word] /= float(total_count)\n",
    "# Formula above calculates the frequencies of words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". in might Hostage the on some performances analyst since , and of it . translate of It to seems not and issued illuminated woods efforts to can must lead , to pie Twenty desk have Roberts around them the the attitude long service crucial in was it Eileen Jesus's of will tackle army the artillery no , For . used . groves who that that which find and but . the broad urban can period water surfaces instantly . press aqueous investigating ; which on for more its being `` from a existence for . to disturbed so ''\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "text = []\n",
    "for x in range(100):\n",
    "    r = random.random()\n",
    "    accumulator = 0.\n",
    "\n",
    "    for word, freq in counts.items():\n",
    "        accumulator += freq\n",
    "\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "print(' '.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability: 4.0648267444860366e-299\n"
     ]
    }
   ],
   "source": [
    "#Probability\n",
    "from operator import mul\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "\n",
    "print('Probability:', reduce(mul, [counts[w] for w in text], 1.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 29\n",
      "Probability: 0.24166666666666667\n"
     ]
    }
   ],
   "source": [
    "from nltk import bigrams, trigrams\n",
    "\n",
    "bigram = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "#Counts\n",
    "for sentence in brown.sents():\n",
    "    for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True):\n",
    "        bigram[w1][w2] += 1\n",
    "print(\"Count:\",bigram[\"charge\"][\"of\"])\n",
    "\n",
    "#Probabilities\n",
    "for w1 in bigram:\n",
    "    total_count = float(sum(bigram[w1].values()))\n",
    "    for w2 in bigram[w1]:\n",
    "        bigram[w1][w2] /= total_count\n",
    "print(\"Probability:\",bigram[\"charge\"][\"of\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 12\n",
      "Probability: 0.41379310344827586\n"
     ]
    }
   ],
   "source": [
    "trigram = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "#Counts\n",
    "for sentence in brown.sents():\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        trigram[(w1, w2)][w3] += 1\n",
    "print(\"Probability:\",trigram[\"charge\", \"of\"][\"the\"]) \n",
    "#Probabilities\n",
    "for w1_w2 in trigram:\n",
    "    total_count = float(sum(trigram[w1_w2].values()))\n",
    "    for w3 in trigram[w1_w2]:\n",
    "        trigram[w1_w2][w3] /= total_count\n",
    " \n",
    "\n",
    "print(\"Count:\",trigram[\"charge\", \"of\"][\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram Prob.: 0.41379310344827586\n",
      "Bigram Prob.: 0.24166666666666667\n",
      "Unigram Prob.: 4.0648267444860366e-299\n"
     ]
    }
   ],
   "source": [
    "print(\"Trigram Prob.:\",trigram[\"charge\", \"of\"][\"the\"])\n",
    "print(\"Bigram Prob.:\",bigram[\"charge\"][\"of\"])\n",
    "print('Unigram Prob.:', reduce(mul, [counts[w] for w in text], 1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability for [0.2, 0.3, 0.5] =  3.569730813535868e-13\n",
      "Probability for [0.2, 0.2, 0.4] =  2.0056787344396662e-13\n",
      "Probability for [0.8, 0.1, 0.1] =  8.56784887613787e-13\n",
      "Probability for [0.1, 0.1, 0.8] =  1.2415378100993134e-13\n",
      "Probability for [0.2, 0.4, 0.4] =  3.569730813535869e-13\n"
     ]
    }
   ],
   "source": [
    "#Exemplary sentence made of words that occur in the corpus\n",
    "sentence_1 = 'Chief of Headquarters .'\n",
    "\n",
    "word_list = sentence_1.split()\n",
    "\n",
    "#Five options of different combination of labda\n",
    "lambdas_1 = [0.2,0.3,0.5]\n",
    "lambdas_2 = [0.2,0.2,0.4]\n",
    "lambdas_3 = [0.8,0.1,0.1]\n",
    "lambdas_4 = [0.1,0.1,0.8]\n",
    "lambdas_5 = [0.2,0.4,0.4]\n",
    "\n",
    "text = [None, None]\n",
    "\n",
    "text_bg = [None]\n",
    "\n",
    "def lambdas_test(word_list, lambdas):\n",
    "    \"\"\"\n",
    "    Input: word_list - a list cosisting of words of string type\n",
    "           lambdas - a list of three float values\n",
    "           \n",
    "    Output: probability for the sequence of words in the word_list parameter\n",
    "    \"\"\"\n",
    "    probability = 1.0\n",
    "    for word in word_list:\n",
    "        if word in counts:\n",
    "            #computes probability according to Interpolation - combining unigram, bigram, and trigram and weighting the models using lambdas\n",
    "            probability *= (lambdas[0]*counts[word] + lambdas[1]*bigram[text_bg[-1]][word] + lambdas[2]*trigram[tuple(text[-2:])][word]) \n",
    "        else:\n",
    "            #taking into account that a word in the test sentence might not appear in the corpus\n",
    "            probability *= 1\n",
    "        \n",
    "    print(\"Probability for\", lambdas,\"= \", probability)\n",
    "\n",
    "lambdas_test(word_list, lambdas_1)\n",
    "lambdas_test(word_list, lambdas_2)\n",
    "lambdas_test(word_list, lambdas_3)\n",
    "lambdas_test(word_list, lambdas_4)\n",
    "lambdas_test(word_list, lambdas_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating random sentences using the Ngram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said was fast your He troubled the Sam . weakness A Jackson this bit with . , the simple an , the him the . so a the . Sheep the merit question interrupt dollars was stayed strange build front sensibly group prevent did was as managed of has with\n"
     ]
    }
   ],
   "source": [
    "# Unigram generating random sentences\n",
    "import random\n",
    "text = []\n",
    "for x in range(50):\n",
    "    r = random.random()\n",
    "    accumulator = 0.\n",
    "\n",
    "    for word, freq in counts.items():\n",
    "        accumulator += freq\n",
    "\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "print(' '.join(text))\n",
    "\n",
    "#As we can see here, the Unigram models returns random words, hence, we cannot expect any meaningful sentences to be generated by this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are characteristically result , when this evening were not lead to life the detail , if only way experience and make .\n"
     ]
    }
   ],
   "source": [
    "# Bigram generating random sentences\n",
    "text = [None]\n",
    " \n",
    "sentence_finished = False\n",
    " \n",
    "while not sentence_finished:\n",
    "    r = random.random()\n",
    "    accumulator = .0\n",
    " \n",
    "    for word in bigram[text[-1]].keys():\n",
    "        accumulator += bigram[text[-1]][word]\n",
    " \n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    " \n",
    "    if text[-1] == None:\n",
    "        sentence_finished = True\n",
    " \n",
    "print (' '.join([t for t in text if t]))\n",
    "# Bigram combines two consequtive words therefore, sentences produced by this model might appear more meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other economic development with emphasis upon the sea .\n"
     ]
    }
   ],
   "source": [
    "#Trigram generating random sentences\n",
    "text = [None, None]\n",
    " \n",
    "sentence_finished = False\n",
    " \n",
    "while not sentence_finished:\n",
    "    r = random.random()\n",
    "    accumulator = .0\n",
    " \n",
    "    for word in trigram[tuple(text[-2:])].keys():\n",
    "        accumulator += trigram[tuple(text[-2:])][word]\n",
    " \n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    " \n",
    "    if text[-2:] == [None, None]:\n",
    "        sentence_finished = True\n",
    " \n",
    "print (' '.join([t for t in text if t]))\n",
    "# Trigram uses two previous words to predict the third one and thus is the most accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compound model random sentence generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even To Those `` It Instead Long But Do Lizzie Leg For For Nobody He Russians This `` A The Ringing The But Some When Via Their As trade Death Any One Pompeii But Maybe Less They I These Within `` But Askington This The The At Before When So The `` The In `` Susan The The Convenience A The Eight `` Large They Since Guidance The Hoag In 6 This We Later Or perhaps During From Mossberg's Quite Matsuo These President There's She Taurog Fla. `` a If Some he The Tell , At Cried A My ( What He If Maria -- Rodding This Susie But That ) Otherwise Now Simmons Because We They In Louis In I So but She Let This Another all In As Thus When `` A Have The In Most Handing Curt With The Weil In The As Athenian The Here's The `` her You I Vivacious Mando Kind The At `` ( Cautious The It Its With A The `` Therefore Others Hudson `` Sure This Well I These Hearing The `` There She's C'est This Was They In We `` six It All He His Our Britain The Taking `` He The Several Thus and And The In The they He These Protests A `` He The One he like He Many Actually I I There It How With He The But The He She They At The Ekstrohm It With `` It'll Thus and Now Even In Miraculously Sometimes And she and Nieman While The yet This But ) Two Dozens They He I You Actually The `` It There Lawrence And ) His In The Unlike `` `` He she Mass ( In Anne O'Banion See While He All This It `` One The Somersaults The The Her He Samuel `` `` `` And A How MacPherson Upon Prior '' And However He Thus A we `` She She Leave It The At The a Have `` Others Microscopic The The He Now Let's A By It Whoever no Four Nonetheless The She Whether How No This Large Mine `` They and Image `` The As 2 But two When and Where ) I Sometimes A For `` I For He How `` Mercer Pity Slowly Fifth `` Those This This `` his In She He Lord Financing The Nouns And Thus To Yesterday What That The Metropolitan Golf's Douglass There to The When `` And The Great What he Since So the The But If And Industrial Pope The `` But Representing Every P.S. The Expands `` And She'd It The There Here ( Headquarters Mr. Elements But The The Somehow First one The They Well New A It Although Straightened It A he and That Thus I These Characteristically With The They Commencing A She `` Mae Petitioner Enough `` `` Further I Overriding This They So In As I've Most World Once Such The One This That's There In Of Evansville He But Turning Let Drunk How Alastor There Benson A Yet Miriam No Heywood Where Included Wildly But We From He Overwhelmed Krim But If His Why Justine `` Yet Sanitation If Advantages He of Nor Ironically It 12 Comic They The She Yes `` Can't The The The Lemma that These When The Radio It He Since She Seven When It No Science This What Visualize This One If Relieved Perhaps But All And `` ) She Few Success He At He Maybe That Although `` Such I Like Mr. Maggie Cholesterol `` Meredith Here One And Westbrook Don't He or The It But Herb Instead Bill After After `` Joel Even By The Then Sergeant One She Here In There Felix It pleasure He It Milton's There It He it To Just `` `` Gilman The Having An He Firm The Schwab Command `` Places but In When Philosophy You Surrounded The The The Cut The And In Perhaps A During Respondents' and Malraux He There We Errors Buaford This and `` `` She I Price All During He Baker With Dulles Were Sixty-seven Such His Patients The If The I'll The `` `` Admiralty This But Your But It To Individual Why Five The For Red There They What's For Someone He I To `` The West You I `` Gauer Taking There As Nothing For National The Even Are or However Mr. She He It Both Specimens Although He Instead For William You Solder He I 5 Try The Parker However The An their We'll What St. Nadine's `` The Nevertheless intelligence There Increasing And `` You People Additional Solder the But And This No And He Her It It `` Sydney Rexroth I Try In Just After Of He She `` It The He Three Following Mrs. Henrietta's Stimulates Phil And It The Vending Watson Wing Indeed You a You've `` Just High Miss A `` Where '' It That He The Yet This she along `` and Then The He Or I Text I Simply His the Brassnose This A , `` Almost Then On This Arms Dried To In Economic ( Most One Had `` There There Future Men Water Casey First `` The to In But Of Most Their The Workshop It Why How We The With He In In Hoag Gains It Do Sake the In Bill Rousseau Like Yet Regency Charlie All A To Threat He But `` Griffith He She The Had We `` Oh The unfortunately Berto When `` With The the Rococo When And Large About What Future `` As `` I This Hence On Therefore Faber's Then The `` Out ) However The He Miss Beadle Location Greg In The ) Backstitching The She Committee I First `` These 3 He One's The they `` Watson Feelers The Sarason Among Since They'll `` Parker Sound At But Somebody Poor And If It The Thus and It This Harmful that So In I'm Above The In Now When He The The John's Such I While This She Now The They Bobby He And Additional On It That They Issue In Toys He He The ( And Miss Who Boys This Before The It Cursing Dill `` There He Fluorescence However The But You Shelter Yet The 7 A This Because There `` But What But She There High It Public Some He I He Under East then Christ's Apparently To In It The Cotton There's Yet Despite `` I How Hmpf A The For Better `` His Verbal He Jemela To His She Much Because Moreover This I The The But Other When Solid Linda by It `` They Of Never Pohl `` Incapable To Congress The If `` But I The Nineteen-sixty All Send It Make Fundamental Applying I `` It Gov. How However In For There `` Arthur Reading 4 and Dr. It This Honest `` Until But He A `` He It she We He like It Stagecoach ( They It 8 Poet the Cannon The `` But These Yet the I Exceptions Adam It that His A Even Cried Argon It She Not She Mr. The Henrietta He `` Rather Walton They And The `` First One 500 The There But A But Now `` The Nearness Time Pay Meeting The Scores I There There Oh He What The Kate The Most The `` When She Will Jubal it Convention but No He We It Lawrence Salvation `` It The Times The and Their Linda The This Boys virtue It It One One I'm There Some Deliberately First Does A Lee Their These `` Part The `` X-ray Honor Extrapolation What Now Bong Remember They're refusal the One `` Yet But He He `` Grumbled Where Two But He Then They Together It not `` Hence Another The Every In In Recovering Efficiency They Escape A In All Again The If An These Mike People The the Signs You The Apparently This disturbing His `` Personally He To I he At Sir And A Fiddles But Bong Now `` Silence The He `` Lee There No But ( but An The If Mimesis If Bones But When It's The He `` By Cotton One There Either Rather They Winston I Johnny He Materials Finally It Applications Kerosene Any A Justine `` In Master The Both Some While Requirements Summary `` -- Score He But Miscellaneous To The Well It It For Image I Even The Miss This Separate Wally's She Offer Let The ) One They The She's It Gross ( She It He Throughout How Solder Eight However to This At An Five It Many Extreme `` Delinquency The I Some It It On Livestock Barco The .\n"
     ]
    }
   ],
   "source": [
    "output = [] \n",
    "sentence_finished = False\n",
    "i = 0 \n",
    "\n",
    "# In this exercise we decided to develop a model that will return a sentence composed of unigram, bigram and trigram products.\n",
    "# Every fifht word in this sentence is a product of the trigram, every tenth is produced by the bigram and all the remaining words are products of the Unigram model.\n",
    "\n",
    "while not sentence_finished:\n",
    "    \n",
    "#Trigram    \n",
    "    if i % 5 == 0:\n",
    "        text1 = [None,None]\n",
    "        r = random.random()\n",
    "        accumulator = .0\n",
    "    \n",
    "        for word in trigram[tuple(text1[-2:])].keys():\n",
    "            accumulator += trigram[tuple(text1[-2:])][word]\n",
    "\n",
    "            if accumulator >= r:\n",
    "                output.append(word)\n",
    "                break\n",
    "#Bigram     \n",
    "    elif i % 10 == 0: \n",
    "        text2 = [None]\n",
    "        r = random.random()\n",
    "        accumulator = .0\n",
    "\n",
    "        for word in bigram[text2[-1]].keys():\n",
    "            accumulator += bigram[text2[-1]][word]\n",
    "\n",
    "            if accumulator >= r:\n",
    "                output.append(word)\n",
    "                break\n",
    "#Unigram                   \n",
    "    else:\n",
    "        for x in range(50):          # The Unigram chooses from 50 words\n",
    "            r = random.random()\n",
    "            accumulator = 0.\n",
    "\n",
    "        for word, freq in counts.items():\n",
    "            accumulator += freq\n",
    "\n",
    "            if accumulator >= r:\n",
    "                output.append(word)\n",
    "                break\n",
    "\n",
    "    if output[-1] == \".\":         # Every sentence has to end with a full stop\n",
    "        sentence_finished = True \n",
    "i += 1          \n",
    "\n",
    "#Output\n",
    "print (' '.join([t for t in output if t]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
